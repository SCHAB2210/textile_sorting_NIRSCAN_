{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL # install pillow - pip install Pillow\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import tensorflow_datasets as tfds # need to install this seperately - pip install tensorflow_datasets\n",
    "import pathlib\n",
    "from skimage import io\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py # pip install h5py\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import socket\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy .csv files from one location to other\n",
    "# use only once when you need to add new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy .csv files from one location to other\n",
    "#use only once when you need to add new class\n",
    "\n",
    "\n",
    "# src_folder = r\"C:\\Users\\ajitj\\OneDrive - Universitetet i Agder\\PhD_Research\\Paper_Writing\\Textile Sorting\\Data_textile_NIRSCAN\\ML_textile_classification_timeseries\\Data_NIRSCAN_CSV\\polyester-000011\\polyester-000011\"\n",
    "# dst_folder = r\"C:\\Users\\ajitj\\OneDrive - Universitetet i Agder\\PhD_Research\\Paper_Writing\\Textile Sorting\\Data_textile_NIRSCAN\\ML_textile_classification_timeseries\\data_ml\\polyester\"\n",
    "\n",
    "# # Search files with .txt extension in source directory\n",
    "# pattern = \"\\*.csv\"\n",
    "# files = glob.glob(src_folder + pattern)\n",
    "\n",
    "# # move the files with txt extension\n",
    "# for file in files:\n",
    "#     # extract file name form file path\n",
    "#     file_name = os.path.basename(file)\n",
    "#     shutil.move(file, dst_folder + file_name)\n",
    "#     #print('Moved:', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this to generate .csv file for ml. Use only once to generate data_cotton_wool_polyester.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to generate a .csv file to be used for ml\n",
    "\n",
    "def read_data(path):\n",
    "    appended_data = []\n",
    "    df = [pd.read_csv(filename,header=21) for filename in glob.glob(path)] \n",
    "    peak_abs=np.zeros((np.shape(df)[0]))\n",
    "    for file in range(np.shape(df)[0]):\n",
    "        #df[file][['wavelength','absorbance', 'reference', 'sample_signal']]=df[file]['data'].str.split(expand=True)\n",
    "        df[file]['wavelength'] = pd.to_numeric(df[file]['Wavelength (nm)'], errors='coerce')\n",
    "        df[file]['absorbance'] = pd.to_numeric(df[file]['Absorbance (AU)'], errors='coerce')\n",
    "        df[file]['absorbance']=df[file]['absorbance']/np.max(df[file]['absorbance'])\n",
    "         # remove data col, as not needed\n",
    "        df[file]=df[file].drop(['wavelength', 'Absorbance (AU)', 'Reference Signal (unitless)', 'Sample Signal (unitless)', 'Wavelength (nm)'], axis=1)\n",
    "        #appended_data.append(df[file]) #appends all values in col\n",
    "        appended_data.append(df[file].T) #228x1 appends each acq in row -- 900 x 228x1\n",
    "        \n",
    "        #peak_abs[file]=df[file]['wavelength'][np.argmax(df[file]['absorbance'])]\n",
    "    appended_data = pd.concat(appended_data) # all 900 acquisition appended together\n",
    "    #appended_data_coton.to_excel('appended.xlsx') # write to csv    \n",
    "    return appended_data, peak_abs\n",
    "\n",
    "\n",
    "# def read_data(path):\n",
    "#     appended_data=[]\n",
    "#     for file in range(np.shape(df)[0]):\n",
    "#         df[file] = pd.DataFrame(df[file])\n",
    "#         #df_cotton[file]=df_cotton[file].drop(['wavelength'], axis=1)\n",
    "#         appended_data.append(df[file]) #228x1\n",
    "#     appended_data = pd.concat(appended_data) # all 900 acquisition appended together\n",
    "#     appended_data.to_csv('df_cotton.csv', index=False)\n",
    "\n",
    "#cotton\n",
    "path_cotton=r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\cotton\\**\\*.csv' # path of file to read\n",
    "appended_data_cotton, peak_abs_cotton = read_data(path_cotton) \n",
    "class_cotton=np.zeros(len(appended_data_cotton)).astype(int)\n",
    "appended_data_cotton.insert(np.shape(appended_data_cotton)[1], \"class\", class_cotton)\n",
    "\n",
    "#wool\n",
    "path_wool=r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\wool\\**\\*.csv'\n",
    "appended_data_wool, peak_abs_wool =read_data(path_wool) # read file 1000x227x4\n",
    "class_wool=np.ones(len(appended_data_wool)).astype(int)\n",
    "appended_data_wool.insert(np.shape(appended_data_wool)[1], \"class\", class_wool)\n",
    "\n",
    "#polyester\n",
    "path_polyester=r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\polyester\\**\\*.csv'\n",
    "appended_data_polyester, peak_abs_polyester =read_data(path_polyester) # read file 1000x227x4\n",
    "class_polyester=np.ones(len(appended_data_polyester)).astype(int) * 2\n",
    "appended_data_polyester.insert(np.shape(appended_data_polyester)[1], \"class\", class_polyester)\n",
    "\n",
    "#check\n",
    "print(\"Cotton data : \\n {}\".format (appended_data_cotton.head()))\n",
    "print(\"Wool data : \\n {}\".format (appended_data_wool.head()))\n",
    "print(\"Polyester data : \\n {}\".format (appended_data_polyester.head()))\n",
    "\n",
    "#concatenate data frames\n",
    "data = [appended_data_cotton, appended_data_wool, appended_data_polyester]\n",
    "data=pd.concat(data) # contains both cottorn and wool with class 0 and 1\n",
    "\n",
    "print(\"data-head : \\n {}\".format (data.head()))\n",
    "print(\"data-tail : \\n {}\".format (data.tail()))\n",
    "\n",
    "#save to csv\n",
    "data.to_csv(r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\data_cotton_wool_polyester.csv', index=False)  # only use when you want to add new class\n",
    "\n",
    "#check\n",
    "data['class'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Ensure 'data' is defined in a previous cell\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m plot_gaussian_fits(\u001b[43mx\u001b[49m, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_gaussian_fits(data, labels):\n",
    "    classes = ['Cotton', 'Wool', 'Polyester']\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_data = data[labels == i]\n",
    "        mean, std_dev = norm.fit(class_data.values.flatten())\n",
    "        plt.hist(class_data.values.flatten(), bins=30, alpha=0.6, label=f'{class_name} Raw Data')\n",
    "        x = np.linspace(min(class_data.values.flatten()), max(class_data.values.flatten()), 100)\n",
    "        plt.plot(x, norm.pdf(x, mean, std_dev) * len(class_data) * np.diff(x)[0], label=f'{class_name} Gaussian Fit')\n",
    "    plt.legend()\n",
    "    plt.title(\"Raw Data and Gaussian Fits\")\n",
    "    plt.xlabel(\"Data Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Ensure 'data' is defined in a previous cell\n",
    "plot_gaussian_fits(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this code to load data_cotton_wool_polyester.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\data_cotton_wool_polyester.csv')\n",
    "#read cotton file, to extract wavelength value. This is same for all acq\n",
    "path_c=r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\cotton\\**\\*.csv' # path of file to read\n",
    "files = glob.glob(path_c)\n",
    "data_c = pd.concat([pd.read_csv(file, header=21) for file in files], ignore_index=True)\n",
    "wavelength = data_c['Wavelength (nm)']\n",
    "wavelength = np.around(wavelength) #truncate to 1 dec place\n",
    "wavelength = wavelength.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cotton_portion=data[0:int(np.shape(data)[0]/3)] #900x229; 900 data samples (acq), 0:227 data, 228 label data frame; data --1800x229\n",
    "wool_portion=data[int(np.shape(data)[0]/3):2*int(np.shape(data)[0]/3)]\n",
    "polyester_portion=data[2*int(np.shape(data)[0]/3)::]\n",
    "\n",
    "#normalize spectral data\n",
    "cp=cotton_portion.iloc[0][0:228]\n",
    "wp=wool_portion.iloc[0][0:228]\n",
    "pp=polyester_portion.iloc[0][0:228]\n",
    "\n",
    "cp_n=(cp-np.min(cp))/(np.max(cp)-np.min(cp))\n",
    "wp_n=(wp-np.min(wp))/(np.max(wp)-np.min(wp))\n",
    "pp_n=(pp-np.min(pp))/(np.max(pp)-np.min(pp))\n",
    "\n",
    "plt.figure(1)\n",
    "#plt.title('Data')\n",
    "plt.plot(wavelength[:228], cp_n, 'g', label='cotton') #cotton_portion.iloc[0][0:228]\n",
    "plt.plot(wavelength[:228], wp_n, 'b', label='wool')\n",
    "plt.plot(wavelength[:228], pp_n, 'r', label='polyester')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Amplitude (a.u)')\n",
    "# Change x-axis tick spacing\n",
    "plt.xticks(np.arange(wavelength[0], wavelength[227], step=80))  # ticks at 0, 2, 4, ..., 10\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp=cotton_portion.iloc[0][0:228]\n",
    "cp_n=(cp-np.mean(cp))/np.std(cp)\n",
    "cp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data(path):\n",
    "#     appended_data = []\n",
    "#     df = [pd.read_csv(filename,header=21) for filename in glob.glob(path)] \n",
    "#     peak_abs=np.zeros((np.shape(df)[0]))\n",
    "#     for file in range(np.shape(df)[0]):\n",
    "#         #df[file][['wavelength','absorbance', 'reference', 'sample_signal']]=df[file]['data'].str.split(expand=True)\n",
    "#         df[file]['wavelength'] = pd.to_numeric(df[file]['Wavelength (nm)'], errors='coerce')\n",
    "#         df[file]['absorbance'] = pd.to_numeric(df[file]['Absorbance (AU)'], errors='coerce')\n",
    "#         df[file]['absorbance']=df[file]['absorbance']/np.max(df[file]['absorbance'])\n",
    "#          # remove data col, as not needed\n",
    "#         df[file]=df[file].drop(['Absorbance (AU)', 'Reference Signal (unitless)', 'Sample Signal (unitless)', 'Wavelength (nm)'], axis=1)\n",
    "#         appended_data.append(df[file])\n",
    "#         peak_abs[file]=df[file]['wavelength'][np.argmax(df[file]['absorbance'])]\n",
    "#     appended_data = pd.concat(appended_data) # all 900 acquisition appended together\n",
    "#     #appended_data_coton.to_excel('appended.xlsx') # write to csv    \n",
    "#     return df, appended_data, peak_abs\n",
    "\n",
    "\n",
    "\n",
    "# def read_data(path):\n",
    "#     append_files=[]\n",
    "#     for file in range(np.shape(df_cotton)[0]):\n",
    "#         df_cotton[file] = pd.DataFrame(df_cotton[file])\n",
    "#         #df_cotton[file]=df_cotton[file].drop(['wavelength'], axis=1)\n",
    "#         append_files_cotton.append(df_cotton[file].T) #228x1\n",
    "#     append_files_cotton = pd.concat(append_files_cotton) # all 900 acquisition appended together\n",
    "#     append_files_cotton.to_csv('df_cotton.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# path_cotton=r'C:\\Users\\ajitj\\OneDrive - Universitetet i Agder\\PhD_Research\\Paper_Writing\\Textile Sorting\\Data_textile_NIRSCAN\\ML_textile_classification_timeseries\\train\\cotton\\*.csv' # path of file to read\n",
    "# df_cotton, appended_data_cotton, peak_abs_cotton =read_data(path_cotton) # read file 1000x227x4\n",
    "# class_cotton=np.zeros(len(appended_data_cotton)).astype(int)\n",
    "# appended_data_cotton.insert(2, \"class\", class_cotton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(df_cotton), np.shape(appended_data_cotton), \n",
    "\n",
    "# append_files_cotton=[]\n",
    "# #convert df_cotton from list to pd dataframe, save it as csv\n",
    "# for file in range(np.shape(df_cotton)[0]):\n",
    "#     df_cotton[file] = pd.DataFrame(df_cotton[file])\n",
    "#     #df_cotton[file]=df_cotton[file].drop(['wavelength'], axis=1)\n",
    "#     append_files_cotton.append(df_cotton[file].T) #228x1\n",
    "# append_files_cotton = pd.concat(append_files_cotton) # all 900 acquisition appended together\n",
    "# append_files_cotton.to_csv('df_cotton.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Use this to read csv files and do ml. Use this ONLY when the data is  saved as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to read csv files and onwards\n",
    "data = pd.read_csv(r'C:\\Users\\chaba\\OneDrive\\Skrivebord\\MAS513_local\\Textile_sorting\\Project\\textile_sorting_NIRSCAN_\\samples\\data_cotton_wool_polyester.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test data set\n",
    "\n",
    "# train_df, val_df = train_test_split(data, test_size=0.20)\n",
    "# train_df, test_df = train_test_split(train_df, test_size=0.1)\n",
    "\n",
    "# #training, val and test data size\n",
    "# print (\"Train shape: {} \\n Val shape: {} \\n Test shape: {} \\n\" .format(np.shape(train_df), np.shape(val_df), np.shape(test_df))) \n",
    "# #np.array( [np.shape(train_df)[0], np.shape(val_df)[0], np.shape(val_df)[0] ]) /np.array([np.shape(data)[0]])\n",
    "# # train, val, test - 70, 20, 10\n",
    "\n",
    "# #save to csv\n",
    "# #data.to_csv('data_cotton_wool.csv', index=False)  \n",
    "\n",
    "# #check\n",
    "# data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split \n",
    "\n",
    "y = data['class']\n",
    "x = data.drop(columns=['class'])\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20,random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=40)\n",
    "\n",
    "\n",
    "print(\"Train: \\n {}\\n\".format(y_train.value_counts()))\n",
    "print(\"Val: \\n {} \\n\".format(y_val.value_counts()))\n",
    "print(\"Test: \\n {} \\n\".format(y_test.value_counts()))\n",
    "\n",
    "print(\"Train: \\n {} {}\\n\".format(np.shape(x_train), np.shape(y_train)))\n",
    "print(\"Val: \\n {} {}\\n\".format(np.shape(x_val), np.shape(y_val)))\n",
    "print(\"Test: \\n {} {}\\n\".format(np.shape(x_test), np.shape(y_test)))\n",
    "\n",
    "# Train: (1440, 228) (1440,)\n",
    "\n",
    "#Val: (360, 228) (360,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check few samples\n",
    "\n",
    "# find where the label is 0 (cotton) and 1 (wool) in training / val  data\n",
    "y_train_1=np.argwhere(y_train==1) # index where y_train=1 wool label\n",
    "y_train_0=np.argwhere(y_train==0) # index where y_train=0 cotton label\n",
    "y_train_2=np.argwhere(y_train==2) # index where y_train=0 cotton label\n",
    "\n",
    "y_val_1=np.argwhere(y_val==1) # index where y_val=1 wool label\n",
    "y_val_0=np.argwhere(y_val==0) # index where y_val=0 cotton label\n",
    "y_val_2=np.argwhere(y_val==2) # index where y_val=0 cotton label\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Data')\n",
    "plt.plot(x_train.iloc[y_train_0[0,0]],'g', label='cotton') #values of absorbance in training data whose label is 0 - cotton\n",
    "plt.plot(x_train.iloc[y_train_1[0,0]],'b', label='wool') #values of absorbance in training data whose label is 1 - wool\n",
    "plt.plot(x_train.iloc[y_train_2[0,0]],'r', label='polyester') #values of absorbance in training data whose label is 1 - wool\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "      tf.keras.metrics.MeanSquaredError(name='Brier score'),\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS):\n",
    "  \n",
    "  # model1 = tf.keras.Sequential([\n",
    "  #     tf.keras.layers.Dense(\n",
    "  #         16, activation='relu',\n",
    "  #         input_shape=(x_train.shape[-1],)),\n",
    "  #     tf.keras.layers.Dropout(0.5),\n",
    "  #     tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "  # ])\n",
    "\n",
    "  model=tf.keras.Sequential([\n",
    "    \n",
    "  #tf.keras.layers.experimental.preprocessing.Rescaling(scale=1 / 127.5, input_shape=(n_row, n_col, 3), offset=-1),\n",
    "  #tf.keras.layers.Dense(228, activation='relu',input_shape=(x_train.shape[-1],)),\n",
    "  tf.keras.layers.Conv1D(8, kernel_size=8, input_shape=(x_train.shape[-1],1), strides=1,  activation='relu'),  \n",
    "  tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "  tf.keras.layers.Conv1D(16, 8, padding=\"same\", activation=\"relu\"),\n",
    "  tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "  \n",
    "  #tf.keras.layers.Conv1D(32, 8, padding=\"same\", activation=\"relu\"),  !!!!!!\n",
    "  \n",
    "  #tf.keras.layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "  #tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "  #tf.keras.layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "  #tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "  #tf.keras.layers.Conv1D(64, 3, activity_regularizer=tf.keras.regularizers.L2(0.01),padding=\"same\", activation=\"relu\"),\n",
    "  tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "  tf.keras.layers.Flatten(),\n",
    "\n",
    "  # tf.keras.layers.Dense(64, \n",
    "  # activity_regularizer=tf.keras.regularizers.L2(0.01),\n",
    "  # activation='relu'),\n",
    "\n",
    "  tf.keras.layers.Dense(\n",
    "     32, \n",
    "      activity_regularizer=tf.keras.regularizers.L2(0.01),\n",
    "      activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  #tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(3, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "  model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # use false when we have softmax at last layer\n",
    "        metrics=metrics)\n",
    "\n",
    "  return model\n",
    "\n",
    "EPOCHS = 100 #50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_prc', \n",
    "#     verbose=1,\n",
    "#     patience=10,\n",
    "#     mode='max',\n",
    "#     restore_best_weights=True)\n",
    "\n",
    "model = make_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path=r'C:\\Users\\devTe\\Desktop\\ML\\log'\n",
    "#ap_name=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#model_save_path=os.path.join(log_path,  ap_name + '.' + 'h5')\n",
    "\n",
    "#log_dir = log_path + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "       #log_dir=log_dir, \n",
    "       histogram_freq=1,\n",
    "       write_graph=False,\n",
    "       write_images=False, #write model weights to visualize as image in TensorBoard.\n",
    "       write_steps_per_second=False,\n",
    "       update_freq='epoch', #'batch'\n",
    "       profile_batch=0,\n",
    "       embeddings_freq=0,\n",
    "       embeddings_metadata=None,\n",
    "       #**kwargs\n",
    ")\n",
    "\n",
    "#convert to one hot encoding\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "#x_train_ohe = enc.fit_transform(np.asarray(x_train).astype('float32').reshape((-1,1))).toarray()\n",
    "#x_val_ohe = enc.fit_transform(np.asarray(x_val).astype('float32').reshape((-1,1))).toarray() \n",
    "y_train_ohe = enc.fit_transform(np.asarray(y_train).astype('float32').reshape((-1,1))).toarray() \n",
    "y_val_ohe = enc.fit_transform(np.asarray(y_val).astype('float32').reshape((-1,1))).toarray()\n",
    "y_test_ohe = enc.fit_transform(np.asarray(y_test).astype('float32').reshape((-1,1))).toarray()\n",
    "\n",
    "\n",
    "history=model.fit(\n",
    " x_train,\n",
    " y_train_ohe,\n",
    " validation_data=(x_val, y_val_ohe),\n",
    " epochs=EPOCHS,\n",
    " verbose=1,\n",
    " callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training history\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  colors=['b', 'g']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "\n",
    "test_predictions_baseline = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "class_names=['Cotton', 'Wool', 'Polyester']\n",
    "threshold=0.8\n",
    "\n",
    "def plot_cm(labels, predictions, class_names, threshold):\n",
    "  predictions > threshold\n",
    "  np.argmax(predictions, axis=1)\n",
    "  predicted_label=np.argmax(predictions, axis=1)\n",
    "  \n",
    "  cm = confusion_matrix(labels,predicted_label ) #predictions > threshold\n",
    "  cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #normalized cm\n",
    "  \n",
    "  plt.figure(figsize=(5,5))\n",
    "  #sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  #plt.subplot(2,1,1)\n",
    "  #sns.heatmap(cmn, annot=True, fmt=\".2f\", xticklabels=class_names, yticklabels=class_names) \n",
    " \n",
    "  # plt.subplot(2,1,2)\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "  \n",
    "  plt.title('Confusion matrix (Test Data set) @ {:.2f} threshold'.format(threshold))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "  print('Classification report : \\n',classification_report(y_test, predicted_label, target_names=class_names)) #true_label, predicted_label\n",
    "\n",
    "baseline_results = model.evaluate(x_val, y_val_ohe,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(y_test, test_predictions_baseline, class_names, threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(x_test)\n",
    "x_test_arr=np.array(x_test)\n",
    "np.shape(x_test_arr)\n",
    "test_predictions_baseline1 = model.predict(x_test_arr[np.random.randint(len(x_test_arr))].reshape(1, -1))\n",
    "print(test_predictions_baseline1)\n",
    "aa=np.argmax(test_predictions_baseline1)\n",
    "\n",
    "# Define the server IP and port (use the Ubuntu machine's IP)\n",
    "SERVER_IP = '192.168.1.20'\n",
    "SERVER_PORT = 12345\n",
    "\n",
    "# Create a socket object\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Connect to the server\n",
    "client_socket.connect((SERVER_IP, SERVER_PORT))\n",
    "\n",
    "# Send the detection label to the server\n",
    "client_socket.send(str(aa).encode('utf-8'))\n",
    "print(f\"Sent label: {aa}\")\n",
    "\n",
    "# Close the connection\n",
    "client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
